import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torch.autograd as autograd
import cPickle
import argparse
import sys
import model
import trainer
import evaluator
import file_namer
from tqdm import tqdm
from build_vocab import Vocabulary
from batch_data import BatchedData
from batched_data_loader import get_loader

def main(args):
  # defining image size
  transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    # PyTorch says images must be normalized like this
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
      std=[0.229, 0.224, 0.225])
  ])

  vocab_path = args.vocab_path if not args.vocab_path == '' else file_namer.make_vocab_name_pkl(args.min_occurrences)
  batched_train_path = args.batched_train_path if not args.batched_train_path == '' else file_namer.make_batch_name_pkl(args.batch_size, True)
  batched_val_path = args.batched_val_path if not args.batched_val_path == '' else file_namer.make_batch_name_pkl(args.batch_size, False)

  with open(vocab_path, 'rb') as f1, open(batched_train_path, 'rb') as f2, open(batched_val_path, 'rb') as f3:
    vocab = cPickle.load(f1)
    batched_train_set = cPickle.load(f2)
    batched_val_set = cPickle.load(f3)
  random_val_loader = get_loader(args.val_image_dir, args.val_caption_path, batched_val_set, vocab, transform, shuffle=True, num_workers=2)

  for i, (images, captions, lengths) in enumerate(random_val_loader):
    if i % 100 == 0 and i > 99:
      print(i)
      if i == 1000:
        break
  for i, (images, captions, lengths) in enumerate(random_val_loader):
    if i % 100 == 0 and i > 99:
      print(i)

if __name__ == '__main__':
  parser = argparse.ArgumentParser()
  parser.add_argument('--train_caption_path', type=str,
                      default='data/annotations/captions_train2014.json',
                      help='Path for train annotation file. Default value of ./data/annotations/captions_train2014.json')
  parser.add_argument('--train_image_dir', type=str,
                      default='data/train2014',
                      help='Path to train image directory. Default value of ./data/train2014')
  parser.add_argument('--batched_train_path', type=str,
                      default='',
                      help='Path to batched train set. Defaults to value generated by file_namer')
  parser.add_argument('--val_caption_path', type=str,
                      default='data/annotations/captions_val2014.json',
                      help='Path for validation annotation file. Default value of ./data/annotations/captions_val2014.json')
  parser.add_argument('--val_image_dir', type=str,
                      default='data/val2014',
                      help='Path to val image directory. Default value of ./data/val2014')
  parser.add_argument('--batched_val_path', type=str,
                      default='',
                      help='Path to batched val set. Defaults to value generated by file_namer')
  parser.add_argument('--vocab_path', type=str,
                      default='',
                      help='Path to vocab. Defaults to value generated by file_namer')
  parser.add_argument('--output_train_name', type=str,
                      default='',
                      help='Output train file name. Defaults to value generated by file_namer')
  parser.add_argument('--output_val_name', type=str,
                      default='',
                      help='Output val file name. Defaults to value generated by file_namer')
  parser.add_argument('--min_occurrences', type=int,
                      default=5,
                      help='Minimum occurrences of a word in the annotations. Default value of 5')
  parser.add_argument('--batch_size', type=int,
                      default=32,
                      help='Size of a batch. Default value of 32')
  parser.add_argument('--num_epochs', type=int,
                      default=10,
                      help='Number of epochs to train for. Default value of 10')
  parser.add_argument('--embedding_dim', type=int,
                      default=512,
                      help='Size of the embedding layer. Default value of 512')
  parser.add_argument('--hidden_size', type=int,
                      default=512,
                      help='Size of the hidden state. Default value of 512')
  parser.add_argument('--encoder_lr', type=float,
                      default=0.001,
                      help='Learning rate for feature mapping layer. Default value of 0.001')
  parser.add_argument('--decoder_lr', type=float,
                      default=0.001,
                      help='Learning rate for decoder. Default value of 0.001')
  parser.add_argument('--grad_clip', type=float,
                      default=5,
                      help='Maximum gradient. Default value of 5')
  parser.add_argument('--dropout', type=float,
                      default=0.0,
                      help='Dropout value for the decoder. Default value of 0.0')
  parser.add_argument('-is_normalized', action='store_true',
                      default=False,
                      help='Set if encoder and decoder are normalized')
  parser.add_argument('-disable_cuda', action='store_true',
                      default=False,
                      help='Set if cuda should not be used')
  parser.add_argument('--load_checkpoint', type=str,
                      default='',
                      help='Saved checkpoint file name. Default behavior is to search using parameters')
  parser.add_argument('-plot', action='store_true',
                      default=False,
                      help='Set if loss should be plotted')
  args = parser.parse_args()
  main(args)
